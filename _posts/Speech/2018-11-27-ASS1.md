---
layout: post
title: Deep Learning Based Approaches to Cocktail Party Problem
categories: [speech]
mathjax: true
---
> *This blog introduces the problem of single-channel Acoustic Source Separation, also known as Cocktail Party Problem and provides a short overview of three current deep learning based techniques that have shown promise.*

## Table of contents
1. [What is Cocktail Party Problem?](#2)
2. [Traditional Approaches](#3)
3. [Mask Learning](#4)
4. [Deep Learning Based Approaches](#5)
	1.  [Deep Clustering](#5.1)
	2. [Deep Attractor Network](#5.2)
	3. [Permutation Invariant Training](#5.3)
5. [Further Readings](#6)
   
## What is Cocktail Party Problem?<a name="2"></a>
Multiple times in the day, while in a meeting, in a bus or even at home, there might be multiple people talking at the same time, yet you will be easily able to understand when you are addressed by a particular person. Machines, unfortunately, are not quite good at it (there are rapidly getting good though). 

This special trait of humans to be able to *attend* to a specific speaker in a multi-speaker environment was first studied in relative detail by Colin Cherry who termed this ['cocktail party effect'](https://en.wikipedia.org/wiki/Cocktail_party_effect) and gave rise to cocktail party problem: given a mixed speech signal y, recover its component speech signals.

In the simplest case, there are only two speakers, and hence, only two component signals $$x_1$$ and $$x_2$$ and y is simple a weighed linear combination of $$x_1$$ and $$x_2$$. i.e. $$ y = \alpha x_1 + \beta x_2$$. 

Typically, this problem is tackled in fequency domain by applying Short Time Fourier Transform (STFT). STFT is a linear operation, hence, in frequency domain, problem becomes: given $$Y = \alpha X_1 + \beta X_2$$ where $$Y,X_1,X_2$$ respectively are the STFT of $$y, x_1, x_2$$.

This problem formulation can be easily extended to the case where there are more than two speakers. 

![Cocktail Party Problem](/images/cocktail_party_problem.png)
*In cocktail party problem, you have to recover the original signals.*


## Traditional Approaches<a name="3"></a>
As the problem is quite old, a number of approaches have been applied to it. 

 - Notable among them are **Computational Auditory Scene Analysis (CASA)**, which basically attempts to mimic human auditory system.
 - **Digital Signal Processing Based Approaches** which assume some distribution over speech signals and try to estimate component signals by maximizing the likelihood.
 - **Decomposition Based Approaches** such as Non-Negative Matrix Factorization (NMF) which perform relatively better among these.
 - **Generative Models** such as Hidden Markov Model (HMM) or Gaussian Mixture Models (GMM) have also been used with modest results.
- **Spectral Clustering** is another approach that was applied to cocktail party problem. See ['Spectral Clustering for speech separation' by Bach and Jordan](https://pdfs.semanticscholar.org/8969/629d891d1234ca7b902944dc145826f46e33.pdf) for more details. 

However, these approaches perform poorly and/or tend not to generalize well to new speakers and/or are computationally extremely expensive and overall have been unable to fully solve Cocktail Party Problem. 

## Mask Learning<a name="4"></a>
As previously mentioned, the problem is typically posed in the frequency domain. Further, it is assumed that phase information does not change *significantly* between $$Y$$ and $$X_i$$ and hence only magnitude spectra of $$X_i$$ is estimated. As opposed to model or algorithm directly outputting the spectrogram of  $$X_i$$, model outputs *n* masks $$M_i$$ where n is the number of speakers. This mask is then applied element-wise to the $$Y$$ to recover spectrogram of component speech signal $$X_i$$. 

These masks are typically of two types:

 - Binary Masks 
 - Amplitude Masks

Binary masks are typically taken to be complementary. It makes the inherent assumption that one Time-Frequency (TF) bin in $$Y$$ either belongs to speaker 1 or speaker 2. If it belongs to speaker 1, corresponding TF bin in $$M_1$$ will be 1, otherwise zero.  

![Binary Mask](/images/binary_mask.png)
*Binary Mask is used to select the TF bins of a particular speaker.*


Amplitude Mask tells how power in a particular TF bin is distributed between both speakers. If a bin equally belongs to both speakers, then both speakers will have 0.5 value in their mask for the corresponding TF bin.

## Deep Learning Based Approaches<a name="5"></a>
It is not quite straightforward how to apply supervised deep learning to this problem because of label assignment problem. In regression based learning, the order of assignment of labels is important and must remain consistent throughout all samples. In the case of two speakers, there are two labels. Suppose sample 1 has two speakers A and B, sample 2 has two speakers B and C and sample 3 has two speakers A and C. In sample 1, A is assigned output label 1 and B is assigned output label 2. In sample 2 then, to maintain consistency B will be assigned output label 2 and C will then be assigned output label 1. In sample 3, if we try to maintain consistency, we run into an impossible situation as both A and C must be assigned output label 1 for that. This problem is termed **label assignment problem** in literature.

Another problem is that generally it is not known how many speakers are there in a mixture, or in other terms, how many output audios should be produced by the model. This is called **dimension mismatch problem**. 

The following image, taken from [Zhuo Chen PhD thesis](https://academiccommons.columbia.edu/doi/10.7916/D8W09C8N) nicely illustrates the problem: 
![Illustration of label assignment and output dimension mismatch problem](/images/problems.png)
*Illustration of label assignment and output dimension mismatch problem*

###  Deep Clustering<a name="5.1"></a>
While people had previously applied DL somewhat successfully to cocktail party problem as well,['Deep clustering: Discriminative embeddings for segmentation and separation'](https://arxiv.org/abs/1508.04306) was the first major breakthrough.

In this paper, the authors' approach is somewhat inspired by Spectral Clustering. They posed the problem as segmentation problem, rather than as a separation problem, and attempted to learn ideal complementary binary masks. But rather than learn the masks directly, they learnt affinity measure by using Deep LSTM Network, which they called Deep Clustering Network (DPCL), which transforms each TF bin into a K-dim embedding. Then by applying simple K-means clustering in embedding space, it is possible to cluster together the TF bins of each speaker from which binary mask can be easily formed. 

![Deep Clustering Network](/images/DC.png)
*Deep Clustering Network*

The loss function used to optimize DPCL is: 
$$ \mathcal{L} =  |VV^T - YY^T|^2_F \ $$
where subscript F stands for Frobneus norm: Eucliadean norm generalized to *n* dimensions.

Intuitively, this loss function pushes the TF bins from same speaker or source together and pulls apart TF bins from different speakers or sources. 

Deep clustering was SOTA at the time. It also showed quite good generalization and even generalized reasonably well to three speaker mixtures even though it was trained on just two speaker mixtures.

### Deep Attractor Network<a name="5.2"></a>
[Deep Attractor Network (DAnet)](https://arxiv.org/abs/1611.08930) improves on the idea of DPCL by introducing attractor points $$A_{c,k} \in \Re^{c \times k}$$ where *c* is the number of speakers (sources) and *k* is the dimension of embedding vector. 
Bins belonging to a particular speaker are *attracted* by its particular attractor point. 

![Deep Attractor Network](/images/DAnet.png)
*Deep Attractor Network Training Scheme. During test time, attractor points from training are used, or K-means is used to cluster the embeddings.*


$$M_i$$ is then constructed by first taking the dot product of attractor point with all embeddings and then summing over the embedding dimension and taking sigmoid of the result. 

Loss function for the DAnet is:
 $$\mathcal{L} =  |X_i - Y * M_i|^2_F$$
 where $$X_i$$ is the label spectrogram, $$Y$$ is the mixed spectrogram which was input to DAnet and $$M_i$$ is the Mask estimated by the DAnet for *ith* speaker.

During testing, either the attractor points from the training are reused or K-Means clustering is applied, same as DPCL. 

### Permutation Invariant Training<a name="5.2"></a>
[Permutation Invariant Training (PIT)](https://arxiv.org/abs/1607.00325) bypasses the label assignment problem by a somewhat brute force approach. It does not assign labels until the model has computed its output. Then it calculates pairwise mean square error between the model outputs and labels for all possible permutations and selects the assignment which gives the least pairwise MSE.

![Permutation Invariant Training](/images/PIT.png)
*PIT network training scheme for two speakers scenario.*


While this approach is attractive because of its simplicity and attains comparable results to DPCL and DAnet, it does suffer from the fact that it can not generalize to more speakers than it has been trained on, something both DPCL and DAnet can easily do.

### Further Readings<a name="6"></a>
This blog just scratches the surface, for more in depth analysis, refer to this [review paper](https://link.springer.com/article/10.1631%2FFITEE.1700814) and [this thesis](https://academiccommons.columbia.edu/doi/10.7916/D8W09C8N) by Columbia's Zhuo Chan which introduces DPCL and DAnet. Also look at this [work by Google](https://looking-to-listen.github.io/) which combines audio-visual features to solve cocktail party problem.







